#   fake_global$free[beta.idx] <- (n_reg*(k-1)+1):(n_reg*k) # Add constrain to free column
#   fake_global$label[beta.idx] <- fake_global$plabel[which(fake_global$op == "~" & fake_global$cluster == k & fake_global$group == 1 + (ngroups*(k-1)))]
# }
#
# # Add constrain to the exogenous covariances per group
# max_free <- max(fake_global$free)
# n_cov_exo <- ((length(exog) * (length(exog) + 1)) / 2)
# for(g in 1:ngroups){
#   exo.idx <- which(fake_global$op == "~~" &
#                    fake_global$group %in% c(g, (ngroups)*(1:nclus) + g) &
#                    fake_global$lhs %in% exog) # Get index of exo cov parameters
#
#   fake_global$free[exo.idx] <- (max_free+(n_cov_exo*(g-1))+1):(max_free+(n_cov_exo*g)) # Add constrain to free column
#
#   fake_global$label[exo.idx] <- fake_global$plabel[which(fake_global$op == "~~" &
#                                                          fake_global$group == g &
#                                                          fake_global$lhs %in% exog)]
# }
#
# # Add free (unconstrained) estimation for the group-cluster-specific endog covariances
# max_free <- max(fake_global$free)
# free_idx <- which(fake_global$free == 0 & fake_global$op == "~~" & fake_global$lhs %in% endog)
# fake_global$free[free_idx] <- (max_free+1):(max_free+length(fake_global$free[free_idx]))
# Add free parameters
free_idx <- which(fake_global$rhs %in% lat_var)
fake_global$free[free_idx] <- 1:length(free_idx)
# Add constraints, "normal" approach
# Start the process to add cluster constraints
# Create a constraint entry in the lavaan format
constraints_row <- data.frame(
id = "", lhs = "", op = "==", rhs = "",
user = 2, block = 0, group = 0, free = 0,
ustart = NA, exo = 0, label = "", plabel = "",
cluster = NA
)
# constraints object refer to regression parameters constraints.
# cons_exo_cov object refer to covariance parameters of exogenous variables (should be group-specific)
# Identify regression parameters
constraints <- fake_global$plabel[which(fake_global$op == "~")] # Get the regression parameters
n_reg <- length(fake_global$plabel[which(fake_global$op == "~" & fake_global$group == 1)]) # Number of reg PER GROUP
# Identify latent variables that are both independent and dependent variables in the model
# Labels of the variance parameters of variables in exo
cons_exo <- fake_global$plabel[which(fake_global$op == "~~" & fake_global$lhs %in% exog)]
# Number of variance parameters that involve variables in exo PER GROUP
n_exo <- length(fake_global$plabel[which(fake_global$op == "~~" &
fake_global$lhs %in% exog & fake_global$group == 1)])
# Create matrices with the necessary constraints entries
constraints_matrix <- constraints_row[rep(
x = 1:nrow(constraints_row),
times = (length(constraints))
), ]
cons_exo_matrix <- constraints_row[rep(
x = 1:nrow(constraints_row),
times = (length(cons_exo))
), ]
rownames(constraints_matrix) <- NULL
rownames(cons_exo_matrix)    <- NULL
# Get a cluster label for all groups (all combinations group*cluster)
clus_label <- rep(x = 1:nclus, each = ngroups)
group_label <- rep(x = 1:ngroups, times = nclus)
# Add cluster labels to the parameter table (not necessary, just for me)
for (j in 1:length(clus_label)) {
fake_global$cluster[fake_global$group == j] <- clus_label[j]
}
# Repeat each cluster label depending on the number of parameters per group.
# i.e. Label each parameter per cluster
reg_labels <- rep(clus_label, each = n_reg) # regressions
exo_labels <- rep(group_label, each = n_exo) # variance of endog1
# Add constraints per cluster (i.e. regression parameters are equal within cluster)
for (k in 1:nclus) {
# Regression constraints
cluster_par <- constraints[reg_labels == k] # Identify regression parameters of cluster k
constraints_matrix[(reg_labels == k), "lhs"] <- cluster_par[1:n_reg] # On the left hand side insert parameters of ONE group
constraints_matrix[(reg_labels == k), "rhs"] <- cluster_par # On the right hand side insert parameters of all groups
}
for(g in 1:ngroups){
# Variances constraints (exo)
group_par_exo <- cons_exo[exo_labels == g]
cons_exo_matrix[(exo_labels == g), "lhs"] <- group_par_exo[1:n_exo]
cons_exo_matrix[(exo_labels == g), "rhs"] <- group_par_exo
}
# Remove redundant constraints (i.e. p1 == p1)
constraints_total <- rbind(constraints_matrix, cons_exo_matrix)
redundant <- which(constraints_total$lhs == constraints_total$rhs) # Identify redundant
constraints_total <- constraints_total[-redundant, ]
rownames(constraints_total) <- NULL
# Bind the model table with the constraints
# browser()
fake_global <- rbind(fake_global, constraints_total)
# Update fake_global with correct order of parameter numbers in the free column
# Now, actually start the global estimation
# Step 2: Iterative EM Algorithm ----
# Create initial random probabilities (z_gks)
# Start using a pre-defined seed for the partition
if (!is.null(seed)) {
set.seed(seed)
}
i <- 0 # iteration initialization
prev_LL <- 0 # previous loglikelihood initialization
diff_LL <- 1 # Set a diff of 1 just to start the while loop
while (diff_LL > 1e-6 & i < max_it) {
i <- i + 1
pi_ks <- colMeans(z_gks)
N_gks <- z_gks * N_gs # Sample size per group-cluster combination
N_gks <- c(N_gks)
# browser()
# M-Step
if (i == 1) {
s2out <- lavaan::sem(
model = fake_global, sample.cov = rep(S_biased, nclus), sample.nobs = N_gks,
baseline = FALSE, se = "none",
h1 = FALSE, check.post = FALSE,
control = list(rel.tol = 1e-06),
sample.cov.rescale = FALSE,
fixed.x = FALSE, ceq.simple = T
)
} else {
# fake_global$ustart <- NA
s2out <- lavaan::sem(
model = fake_global, sample.cov = rep(S_biased, nclus), sample.nobs = N_gks,
baseline = FALSE,
h1 = FALSE, check.post = FALSE,
control = list(rel.tol = 1e-06), start = start,
sample.cov.rescale = FALSE,
fixed.x = FALSE, ceq.simple = T
) # , control = list(max.iter = 50))
}
start <- coef(s2out)
# E-Step
# Prepare the log likelihood used in the E-step
# Get the log likelihood from s2out results
# 1. Estimate log-likelihood using Lavaan
gk <- 0
global_loglik_gks <- matrix(data = 0, nrow = ngroups, ncol = nclus)
global_loglik_gksw <- matrix(data = 0, nrow = ngroups, ncol = nclus)
global_LL <- 0
for (k in 1:nclus) {
for (g in 1:ngroups) {
# browser()
gk <- gk + 1L
global_loglik_gk <- lavaan:::lav_mvnorm_loglik_samplestats(
sample.mean = rep(0, length(vars)), # s2out@SampleStats@mean[[gk]],
sample.nobs = N_gs[g], # Use original sample size to get the correct loglikelihood
sample.cov  = S_biased[[g]], # s2out@SampleStats@cov[[gk]],
Mu          = rep(0, length(vars)), # s2out@SampleStats@mean[[gk]],
Sigma       = s2out@implied$cov[[gk]]
)
global_loglik_gks[g, k] <- global_loglik_gk
global_loglik_gksw[g, k] <- log(pi_ks[k]) + global_loglik_gk # weighted loglik
}
}
# Get total loglikelihood
# First, deal with arithmetic underflow by subtracting the maximum value per group
max_gs <- apply(global_loglik_gksw, 1, max) # Get max value per row
minus_max <- sweep(x = global_loglik_gksw, MARGIN = 1, STATS = max_gs, FUN = "-") # Subtract the max per row
exp_loglik <- exp(minus_max) # Exp before summing for total loglikelihood
loglik_gsw <- log(apply(exp_loglik, 1, sum)) # Sum exp_loglik per row and then take the log again
global_LL <- sum((loglik_gsw + max_gs)) # Add the maximum again and then sum them all for total loglikelihood
# Now, do E-step
E_out <- EStep(
pi_ks = pi_ks, ngroup = ngroups,
nclus = nclus, loglik = global_loglik_gks
)
# browser()
z_gks <- E_out
diff_LL <- abs(global_LL - prev_LL)
prev_LL <- global_LL
# print(LL)
# print(diff_LL)
}
# Extract matrices from final step 2 output
EST_s2      <- lavaan::lavInspect(s2out, "est", add.class = TRUE, add.labels = TRUE) # Estimated matrices step 2
beta_gks    <- lapply(EST_s2, "[[", "beta")
psi_gks_tmp <- lapply(EST_s2, "[[", "psi")
# Select useful betas
k.idx <- (seq_len(nclus) - 1) * ngroups + 1L
beta_ks <- beta_gks[k.idx]
# Re-order betas
if (nclus == 1) {
beta_ks <- beta_ks[[1]]
beta_ks <- reorder(beta_ks)
} else if (nclus != 1) {
beta_ks <- lapply(1:nclus, function(x) {
reorder(beta_ks[[x]])
}) # Does not work with only one cluster
}
# Re-order psi
# Put them in a matrix of matrices
psi_gks <- matrix(data = list(NA), nrow = ngroups, ncol = nclus)
for(gk in 1:(ngroups*nclus)){
psi_gks[[gk]] <- psi_gks_tmp[[gk]]
}
}
# MODEL SELECTION
# Get observed data log-likelihood for model selection purposes)
Sigma_gks <- matrix(data = list(NA), nrow = ngroups, ncol = nclus)
Obs.loglik_gks <- matrix(data = 0, nrow = ngroups, ncol = nclus)
Obs.loglik_gksw <- matrix(data = 0, nrow = ngroups, ncol = nclus)
pi_ks <- colMeans(z_gks)
for (k in 1:nclus) {
ifelse(test = (nclus == 1), yes = (beta <- beta_ks), no = (beta <- beta_ks[[k]]))
for (g in 1:ngroups) {
S_biased <- S_unbiased[[g]] * (N_gs[[g]] - 1) / N_gs[[g]]
var_eta <- solve(I - beta) %*% psi_gks[[g, k]] %*% t(solve(I - beta))
Sigma_gks[[g, k]] <- lambda_gs[[g]] %*% var_eta %*% t(lambda_gs[[g]]) + theta_gs[[g]]
Sigma[[g, k]] <- 0.5 * (Sigma[[g, k]] + t(Sigma[[g, k]]))
# Sigma[[g, k]][lower.tri(Sigma[[g, k]])] <- t(Sigma[[g, k]])[lower.tri(Sigma[[g, k]])]
Obs.loglik_gk <- lavaan:::lav_mvnorm_loglik_samplestats(
sample.mean = rep(0, length(vars)),
sample.nobs = N_gs[g], # Use original sample size to get the correct loglikelihood
# sample.nobs = N_gks[g, k],
sample.cov  = S_biased, # Item (observed) covariance matrix from step 1
Mu          = rep(0, length(vars)),
Sigma       = Sigma_gks[[g, k]] # Item (observed) covariance matrix from step 2
)
Obs.loglik_gks[g, k] <- Obs.loglik_gk
Obs.loglik_gksw[g, k] <- log(pi_ks[k]) + Obs.loglik_gk
}
}
# Get total observed loglikelihood
# First, deal with arithmetic underflow by subtracting the maximum value per group
Obs.max_gs <- apply(Obs.loglik_gksw, 1, max) # Get max value per row
Obs.minus_max <- sweep(x = Obs.loglik_gksw, MARGIN = 1, STATS = Obs.max_gs, FUN = "-") # Subtract the max per row
Obs.exp_loglik <- exp(Obs.minus_max) # Exp before summing for total loglikelihood
Obs.loglik_gsw <- log(apply(Obs.exp_loglik, 1, sum)) # Sum exp_loglik per row and then take the log again
Obs.LL <- sum((Obs.loglik_gsw + Obs.max_gs)) # Add the maximum again and then sum them all for total loglikelihood
# Calculate BIC for model selection
# Four types of BIC:
# (1) BIC(N) based on the log-likelihood from the factors
# (2) BIC(G) based on the log-likelihood from the factors
# (3) BIC(N) based on the log-likelihood from the observed data
# (4) BIC(G) based on the log-likelihood from the observed data
# Get important values
Q <- length(lat_var)
J <- length(vars)
# Structural parameters
ifelse(test = (nclus == 1), yes = (n_reg <- sum(beta_ks != 0)), no = (n_reg <- sum(beta_ks[[1]] != 0)))
Q_exo <- length(exog)
n_cov_exo <- ((Q_exo * (Q_exo + 1)) / 2)
Q_endo1 <- length(endog1)
Q_endo2 <- length(endog2)
n_cov_endo2 <- ((Q_endo2 * (Q_endo2 + 1)) / 2)
# Measurement parameters
n_res <- sum(theta_gs[[1]] != 0)
n_load <- sum(lambda_gs[[1]] != 0)
# How many free loadings?
# Identify the free loadings using the parameter table from lavaan
n_free <- 0
if (is.list(S1)) {
for (m in 1:m) {
partbl    <- lavaan::parTable(S1output[[m]])
free_load <- which(partbl$op == "=~" & is.na(partbl$ustart) & partbl$group == 1 & partbl$label != partbl$plabel)
n_free    <- n_free + length(free_load)
}
} else if (!is.list(S1)) {
partbl    <- lavaan::parTable(S1output)
free_load <- which(partbl$op == "=~" & is.na(partbl$ustart) & partbl$group == 1 & partbl$label != partbl$plabel)
n_free    <- length(free_load)
}
# Get the correct number of free parameters depending on the possible combinations
# browser()
if (endo_group_specific == F) { # Is endogenous covariance group-specific?
nr_par_factors <- (nclus - 1) + (n_reg * nclus) + (n_cov_exo * ngroups) + (Q_endo1 * nclus) + (n_cov_endo2 * nclus)
nr_pars <- (nclus - 1) + (n_reg * nclus) + (n_cov_exo * ngroups) + (Q_endo1 * nclus) + (n_cov_endo2 * nclus) + (n_res * ngroups) + (n_load - Q - n_free) + (n_free * ngroups)
} else if (endo_group_specific == T) {
nr_par_factors <- (nclus - 1) + (n_reg * nclus) + (n_cov_exo * ngroups) + (Q_endo1 * ngroups) + (n_cov_endo2 * ngroups)
nr_pars <- (nclus - 1) + (n_reg * nclus) + (n_cov_exo * ngroups) + (Q_endo1 * ngroups) + (n_cov_endo2 * ngroups) + (n_res * ngroups) + (n_load - Q - n_free) + (n_free * ngroups)
}
# Calculate BIC
# Observed
Obs.BIC_N <- (-2 * Obs.LL) + (nr_pars * log(sum(N_gs)))
Obs.BIC_G <- (-2 * Obs.LL) + (nr_pars * log(ngroups))
# Factors
BIC_N <- (-2 * LL) + (nr_par_factors * log(sum(N_gs)))
BIC_G <- (-2 * LL) + (nr_par_factors * log(ngroups))
# Calculate AIC (and AIC3).
# Observed
Obs.AIC  <- (-2 * Obs.LL) + (nr_pars * 2)
Obs.AIC3 <- (-2 * Obs.LL) + (nr_pars * 3)
# Factors
AIC  <- (-2 * LL) + (nr_par_factors * 2)
AIC3 <- (-2 * LL) + (nr_par_factors * 3)
# Calculate entropy and ICL
# Entropy
# Code from github user daob (Oberski, 2019): https://gist.github.com/daob/c2b6d83815ddd57cde3cebfdc2c267b3
# p is the prior or posterior probabilities
entropy <- function(p) {
p <- p[p > sqrt(.Machine$double.eps)] # since Lim_{p->0} p log(p) = 0
sum(-p * log(p))
}
# browser()
sum_entropy <- sum(apply(z_gks, 1, entropy)) # Total entropy
# Entropy R2
entropy.R2 <- function(pi, post) {
error_prior <- entropy(pi) # Class proportions
error_post <- mean(apply(post, 1, entropy))
R2_entropy <- (error_prior - error_post) / error_prior
R2_entropy
}
R2_entropy <- entropy.R2(pi = pi_ks, post = z_gks)
# browser()
# ICL
ICL     <- BIC_G + (sum_entropy * 2)
Obs.ICL <- Obs.BIC_G + (sum_entropy * 2)
# Re order matrices so that we get them in the following order:
# (1) Exogenous latent variables
# (2) Endogenous latent variables: independent and dependent variables at the same time
# (3) Endogenous latent variables: only dependent variables
# Reoder psi_ks and beta_ks by using the reorder function in the lapply function
psi_gks <- array(lapply(1:gro_clu, function(x) {
reorder(psi_gks[[x]])
}), dim = c(ngroups, nclus))
if (nclus == 1) {
beta_ks <- reorder(beta_ks)
} else if (nclus != 1) {
beta_ks <- lapply(1:nclus, function(x) {
reorder(beta_ks[[x]])
}) # Does not work with only one cluster
}
names(beta_ks) <- paste("Cluster", seq_len(nclus))
return(list(
posteriors    = z_gks,
final_fit     = s2out, # Final fit of step 2 (contains all group-cluster combinations)
MM            = S1output, # Output of step 1 (measurement model)
param         = list(psi_gks = psi_gks, lambda = lambda_gs, # Lambda is invariant across all groups
theta = theta_gs, beta_ks = beta_ks, cov_eta = cov_eta), # Factor covariance matrix from first step
logLik        = list(loglik        = LL, # Final logLik of the model (its meaning depends on argument "sam_method")
global_loglik = ifelse(test = sam_method == "global", global_LL, NA), # Only valid if sam_method = "global"
loglik_gksw   = loglik_gksw, # Weighted logLik per group-cluster combinations
runs_loglik   = loglik_nstarts, # loglik for each start
obs_loglik    = Obs.LL), # Only useful if fit = "local"
model_sel     = list(BIC        = list(observed = list(BIC_N = Obs.BIC_N, BIC_G = Obs.BIC_G),
Factors = list(BIC_N = BIC_N, BIC_G = BIC_G)),
AIC        = list(observed = Obs.AIC, Factors = AIC),
AIC3       = list(observed = Obs.AIC3, Factors = AIC3),
R2_entropy = R2_entropy,
ICL        = list(observed = Obs.ICL, Factors = ICL)),
sample.stats  = list(S = S_unbiased, n_cov_exo = n_cov_exo),
NrPar         = list(Obs.nrpar = nr_pars, Fac.nrpar = nr_par_factors),
N_gs          = N_gs
))
}
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
vars
S1 <- list('
# factor loadings
F1 =~ x1 + x2 + x3 + x4 + x5',
'F2 =~ z1 + z2 + z3 + z4 + z5',
'F3 =~ m1 + m2 + m3 + m4 + m5',
'F4 =~ y1 + y2 + y3 + y4 + y5
')
S1
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
vars
Step1_args
do.call(what = Step1, args = Step1_args)
Step1 <- function(S1 = S1, s1_fit = s1_fit, centered = centered,
group = group, S_unbiased = S_unbiased, ...){
# Step 1: Get group-specific factor covariances
# Perform Step 1 according to the number of measurement blocks
browser()
if (is.list(S1)) { # Do we have measurement blocks?
M <- length(S1) # How many measurement blocks?
if (!is.null(s1_fit)) {
# If the user inputs their own step 1 results, use it
S1output <- s1_fit
} else if (is.null(s1_fit)) {
# If not, estimate step 1 using cfa()
S1output <- vector(mode = "list", length = length(S1))
for (m in 1:M) {
# Estimate one cfa per measurement block
S1output[[m]] <- lavaan::cfa(
model = S1[[m]], data = centered, group = group,
se = "none", test = "none",
baseline = FALSE, h1 = FALSE,
implied = FALSE, loglik = FALSE,
...
)
}
}
# How many groups?
ngroups <- lavInspect(S1output[[1]], "ngroups")
# Extract measurement parameters per measurement block
# Extract Lambda & Theta for each group in all blocks
# Initialize lists to store lambdas and thetas per block
lambda_block <- vector(mode = "list", length = M)
theta_block  <- vector(mode = "list", length = M)
for (m in 1:M) {
EST_block         <- lavaan::lavInspect(S1output[[m]], "est")
lambda_block[[m]] <- lapply(X = EST_block, "[[", "lambda")
theta_block[[m]]  <- lapply(X = EST_block, "[[", "theta")
}
# Put together lambda & theta for all groups
# We should end with one lambda and theta matrix per group
lambda_group <- vector(mode = "list", length = ngroups)
theta_group  <- vector(mode = "list", length = ngroups)
for (g in 1:ngroups) {
for (m in 1:M) { # Put matrices of the same group in the same list
lambda_group[[g]][[m]] <- lambda_block[[m]][[g]]
theta_group[[g]][[m]]  <- theta_block[[m]][[g]]
}
# Put together the matrices per group
# Lambda
lambda_group[[g]] <- lavaan::lav_matrix_bdiag(lambda_group[[g]])
# Theta
theta_group[[g]]  <- lavaan::lav_matrix_bdiag(theta_group[[g]])
# Label correctly the rows and columns of the resulting matrices
# Lambda
rownames(lambda_group[[g]]) <- vars
colnames(lambda_group[[g]]) <- lat_var
# Theta
rownames(theta_group[[g]]) <- colnames(theta_group[[g]]) <- vars
}
# Change names and get matrices/values relevant for future steps
lambda_gs <- lambda_group
theta_gs  <- theta_group
N_gs      <- lavaan::lavInspect(S1output[[1]], "nobs") # nobs per group
# Estimate cov_eta (Covariance between the factors)
M_mat   <- vector(mode = "list", length = ngroups) # M matrices from SAM
cov_eta <- vector(mode = "list", length = ngroups)
for (g in 1:ngroups) {
# Compute the M (mapping) matrix in case we have different blocks
lambda_g <- lambda_gs[[g]]
theta_g  <- theta_gs[[g]]
M_mat[[g]] <- solve(t(lambda_g) %*% solve(theta_g) %*% lambda_g) %*% t(lambda_g) %*% solve(theta_g)
# Get the covariance of the factors (cov_eta)
# First, get biased sample covariance matrix per group (S)
S <- S_unbiased[[g]] * (N_gs[[g]] - 1) / N_gs[[g]]
cov_eta[[g]] <- M_mat[[g]] %*% (S - theta_g) %*% t(M_mat[[g]])
}
} else if (!is.list(S1)) {
# If not a list, then we only have one measurement block (all latent variables at the same time)
if (!is.null(s1_fit)) {
# If the user input their own step 1 results, use it
S1output <- s1_fit
} else if (is.null(s1_fit)) {
S1output <- lavaan::cfa(
model = S1, data = centered, group = group,
se = "none", test = "none",
baseline = FALSE, h1 = FALSE,
implied = FALSE, loglik = FALSE,
...
)
}
# Define some important objects
# How many groups?
ngroups <- lavaan::lavInspect(S1output, "ngroups")
N_gs    <- lavaan::lavInspect(S1output, "nobs") # nobs per group
# all estimated model matrices, per group
EST       <- lavaan::lavInspect(S1output, "est", add.class = FALSE, add.labels = TRUE)
theta_gs  <- lapply(EST, "[[", "theta")
lambda_gs <- lapply(EST, "[[", "lambda")
cov_eta   <- lapply(EST, "[[", "psi") # cov_eta name refers to Variance of eta (eta being the latent variables)
}
# Biased cov matrix
S_biased <- vector(mode = "list", length = ngroups)
for(g in 1:ngroups){S_biased[[g]] <- S_unbiased[[g]] * (N_gs[[g]] - 1) / N_gs[[g]]}
# Return all the useful objects
return(list(
S1output  = S1output,
lambda_gs = lambda_gs,
theta_gs  = theta_gs,
cov_eta   = cov_eta,
ngroups   = ngroups,
N_gs      = N_gs,
S_biased  = S_biased
))
}
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
vars
ngroups
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
traceback()
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
M
vars
ngroups
g
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
n
source("~/GitHub/MMG-SEM/R/MMG-SEM.R", echo=TRUE)
# Run MMG-SEM
# Continuous (Run normally as usual)
fit.con <- MMGSEM(dat = Data$SimData, S1 = S1, S2 = S2, group = "group", nclus = 2, seed = 1,
nstarts = 20, ordered = F, group.equal = "loadings", group.partial = NonInv)
fit.con$posteriors
