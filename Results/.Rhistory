# summary(model.sem, rsquare = T)
# cor(Data)
# Make categorical
Data <- matrix(data = NA, nrow = 1000, ncol = 9)
Data[, 1] <- as.numeric(cut(facsc[, 1], breaks = c(-Inf, thresh1, Inf)))
Data[, 2] <- as.numeric(cut(facsc[, 1], breaks = c(-Inf, thresh2, Inf)))
Data[, 3] <- as.numeric(cut(facsc[, 1], breaks = c(-Inf, thresh3, Inf)))
Data[, 4] <- as.numeric(cut(facsc[, 2], breaks = c(-Inf, thresh4, Inf)))
Data[, 5] <- as.numeric(cut(facsc[, 2], breaks = c(-Inf, thresh5, Inf)))
Data[, 6] <- as.numeric(cut(facsc[, 2], breaks = c(-Inf, thresh6, Inf)))
Data[, 7] <- as.numeric(cut(facsc[, 3], breaks = c(-Inf, thresh7, Inf)))
Data[, 8] <- as.numeric(cut(facsc[, 3], breaks = c(-Inf, thresh8, Inf)))
Data[, 9] <- as.numeric(cut(facsc[, 3], breaks = c(-Inf, thresh9, Inf)))
colnames(Data) <- paste0(rep("x", 9), 1:9)
# Data[, 1] <- as.numeric(cut(Data[, 1], breaks = c(-Inf, thresh1, Inf)))
# Data[, 2] <- as.numeric(cut(Data[, 2], breaks = c(-Inf, thresh2, Inf)))
# Data[, 3] <- as.numeric(cut(Data[, 3], breaks = c(-Inf, thresh3, Inf)))
# Data[, 4] <- as.numeric(cut(Data[, 4], breaks = c(-Inf, thresh4, Inf)))
# Data[, 5] <- as.numeric(cut(Data[, 5], breaks = c(-Inf, thresh5, Inf)))
# Data[, 6] <- as.numeric(cut(Data[, 6], breaks = c(-Inf, thresh6, Inf)))
# Data[, 7] <- as.numeric(cut(Data[, 7], breaks = c(-Inf, thresh7, Inf)))
# Data[, 8] <- as.numeric(cut(Data[, 8], breaks = c(-Inf, thresh8, Inf)))
# Data[, 9] <- as.numeric(cut(Data[, 9], breaks = c(-Inf, thresh9, Inf)))
# cor(Data)
# lavCor(Data, ordered = T)
# Check lavaan vs mirt
model.sem.cat  <- sem(model = model, data = Data, ordered = T, std.lv = F)
# summary(model.sem.cat, rsquare = T)
lavInspect(model.sem.cat, what = "est")$beta
model.mirt <- mirt(Data,
model = "F1 = 1-3,
F2 = 4-6,
F3 = 7-9",
itemtype = "graded")
fs <- fscores(model.mirt) #, full.scores.SE = T)
lm(F1 ~ F2 + F3, data = as.data.frame(fs))
lm(f1 ~ f2 + f3, data = as.data.frame(predict(model.sem.cat)))
# Lavaan continuous
model.sem.con  <- sem(model = model, data = Data, ordered = F, std.lv = F)
# summary(model.sem.con)
lavInspect(model.sem.con, what = "est")$beta
quantile(rnorm(100, 1), .5)
quantile(rnorm(100, 1), c(0, .25, .5, .75, 1)
)
quantile(rnorm(1000, 1), c(0, .25, .5, .75, 1))
quantile(rnorm(1000, 0, 1), c(0, .25, .5, .75, 1))
1/6
1/5
1/4
quantile(rnorm(1000, 0, 1), c(.2, .4, .6, .8))
quantile(rnorm(1000, 0, 1), c(.5))
quantile(rnorm(1000, 0, 1), c(.3))
quantile(rnorm(1000, 0, 1), c(.2))
quantile(rnorm(1000, 0, 1), c(.6))
quantile(rnorm(1000, 0, 1), c(.9))
quantile(rnorm(1000, 0, 1), c(.8))
install.packages("brms")
library(tidyverse) # needed for data manipulation and plotting
library(rstan)
install.packages("bayesplot")
(
)
library(tidyverse) # needed for data manipulation and plotting
library(rstan)
library(brms)
library(psych) #to get some extended summary statistics
library(bayesplot) #  needed for plotting
library(ggmcmc)
install.packages("ggmcmc")
library(ggmcmc)
library(mcmcplots)
install.packages("mcmcplots")
library(mcmcplots)
library(tidybayes)
install.packages("tidybayes")
library(tidybayes)
install.packages("GGally")
# Load the data
popular2data <- read_sav(file = "https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true")
# Select relevant variables
popular2data <- select(popular2data, pupil, class, extrav, sex, texp, popular) # we select just the variables we will use
head(popular2data) # we have a look at the first 6 observations
# Visually check some relations
ggplot(data  = popular2data,
aes(x = extrav,
y = popular))+
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
geom_smooth(method = lm,
se     = FALSE,
col    = "black",
linewidth   = .5,
alpha  = .8)+ # to add regression line
theme_minimal()+
labs(title    = "Popularity vs. Extraversion",
subtitle = "add regression line")
# Do it again, but different colors for each class (multilevel stuff)
ggplot(data      = popular2data,
aes(x     = extrav,
y     = popular,
col   = class,
group = class))+ #to add the colours for different classes
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
theme_minimal()+
theme(legend.position = "none")+
scale_color_gradientn(colours = rainbow(100))+
geom_smooth(method = lm,
se     = FALSE,
size   = .5,
alpha  = .8)+ # to add regression line
labs(title    = "Popularity vs. Extraversion",
subtitle = "add colours for different classes and regression lines")
# Which classes are more extreme?
# Create function for this
f1 <- function(data, x, y, grouping, n.highest = 3, n.lowest = 3){
groupinglevel <- data[,grouping]
res           <- data.frame(coef = rep(NA, length(unique(groupinglevel))), group = unique(groupinglevel))
names(res)    <- c("coef", grouping)
for(i in 1:length(unique(groupinglevel))){
data2    <- as.data.frame(data[data[,grouping] == i,])
res[i,1] <- as.numeric(lm(data2[, y] ~ data2[, x])$coefficients[2])
}
top    <- res %>% top_n(n.highest, coef)
bottom <- res %>% top_n(-n.lowest, coef)
res    <- res %>% mutate(high_and_low = ifelse(coef %in% top$coef, "top",  ifelse(coef %in% bottom$coef, "bottom", "none")))
data3  <- left_join(data, res)
return(data3)
}
# Use the function and highlight the more extreme groups
f1(data = as.data.frame(popular2data),
x    = "extrav",
y    = "popular",
grouping = "class",
n.highest = 3,
n.lowest = 3) %>%
ggplot()+
geom_point(aes(x     = extrav,
y     = popular,
fill  = class,
group = class),
size     =  1,
alpha    = .5,
position = "jitter",
shape    = 21,
col      = "white")+
geom_smooth(aes(x     = extrav,
y     = popular,
col   = high_and_low,
group = class,
size  = as.factor(high_and_low),
alpha = as.factor(high_and_low)),
method = lm,
se     = FALSE)+
theme_minimal()+
theme(legend.position = "none")+
scale_fill_gradientn(colours = rainbow(100))+
scale_color_manual(values=c("top"      = "blue",
"bottom"   = "red",
"none"     = "grey40"))+
scale_size_manual(values=c("top"       = 1.2,
"bottom"   = 1.2,
"none"     = .5))+
scale_alpha_manual(values=c("top"      = 1,
"bottom"    = 1,
"none"      =.3))+
labs(title="Linear Relationship Between Popularity and Extraversion for 100 Classes",
subtitle="The 6 with the most extreme relationship have been highlighted red and blue")
##### ----------------------------------------------------------------------------------------------
# Start estimating bayesian multilevel models
# First, intercept only
interceptonlymodeltest <- brm(popular ~ 1 + (1 | class),
data   = popular2data,
warmup = 100,
iter   = 200,
chains = 2,
init  = "random",
cores  = 2)  #the cores function tells STAN to make use of 2 CPU cores simultaneously instead of just 1.
summary(interceptonlymodeltest)
# Add more iterations
interceptonlymodel <- brm(popular ~ 1 + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(interceptonlymodel)
hyp <- "sd_class__Intercept^2 / (sd_class__Intercept^2 + sigma^2) = 0"
hypothesis(interceptonlymodel, hyp, class = NULL)
# Second, add the other first-level predictors (individual level)
model1 <- brm(popular ~ 1 + sex + extrav + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(model1)
model1tranformed <- ggs(model1) # cannot use
mcmc_plot(model1, type = "trace")
library(tidyverse) # needed for data manipulation.
library(brms) # for the analysis
library(haven) # to load the SPSS .sav file
library(RColorBrewer) # needed for some extra colours in one of the graphs
library(ggmcmc)
library(ggthemes)
library(ggridges)
# Load the data
popular2data <- read_sav(file = "https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true")
# Select relevant variables
popular2data <- select(popular2data, pupil, class, extrav, sex, texp, popular) # we select just the variables we will use
head(popular2data) # we have a look at the first 6 observations
# Visually check some relations
ggplot(data  = popular2data,
aes(x = extrav,
y = popular))+
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
geom_smooth(method = lm,
se     = FALSE,
col    = "black",
linewidth   = .5,
alpha  = .8)+ # to add regression line
theme_minimal()+
labs(title    = "Popularity vs. Extraversion",
subtitle = "add regression line")
# Do it again, but different colors for each class (multilevel stuff)
ggplot(data      = popular2data,
aes(x     = extrav,
y     = popular,
col   = class,
group = class))+ #to add the colours for different classes
geom_point(size     = 1.2,
alpha    = .8,
position = "jitter")+ #to add some random noise for plotting purposes
theme_minimal()+
theme(legend.position = "none")+
scale_color_gradientn(colours = rainbow(100))+
geom_smooth(method = lm,
se     = FALSE,
size   = .5,
alpha  = .8)+ # to add regression line
labs(title    = "Popularity vs. Extraversion",
subtitle = "add colours for different classes and regression lines")
# Which classes are more extreme?
# Create function for this
f1 <- function(data, x, y, grouping, n.highest = 3, n.lowest = 3){
groupinglevel <- data[,grouping]
res           <- data.frame(coef = rep(NA, length(unique(groupinglevel))), group = unique(groupinglevel))
names(res)    <- c("coef", grouping)
for(i in 1:length(unique(groupinglevel))){
data2    <- as.data.frame(data[data[,grouping] == i,])
res[i,1] <- as.numeric(lm(data2[, y] ~ data2[, x])$coefficients[2])
}
top    <- res %>% top_n(n.highest, coef)
bottom <- res %>% top_n(-n.lowest, coef)
res    <- res %>% mutate(high_and_low = ifelse(coef %in% top$coef, "top",  ifelse(coef %in% bottom$coef, "bottom", "none")))
data3  <- left_join(data, res)
return(data3)
}
# Use the function and highlight the more extreme groups
f1(data = as.data.frame(popular2data),
x    = "extrav",
y    = "popular",
grouping = "class",
n.highest = 3,
n.lowest = 3) %>%
ggplot()+
geom_point(aes(x     = extrav,
y     = popular,
fill  = class,
group = class),
size     =  1,
alpha    = .5,
position = "jitter",
shape    = 21,
col      = "white")+
geom_smooth(aes(x     = extrav,
y     = popular,
col   = high_and_low,
group = class,
size  = as.factor(high_and_low),
alpha = as.factor(high_and_low)),
method = lm,
se     = FALSE)+
theme_minimal()+
theme(legend.position = "none")+
scale_fill_gradientn(colours = rainbow(100))+
scale_color_manual(values=c("top"      = "blue",
"bottom"   = "red",
"none"     = "grey40"))+
scale_size_manual(values=c("top"       = 1.2,
"bottom"   = 1.2,
"none"     = .5))+
scale_alpha_manual(values=c("top"      = 1,
"bottom"    = 1,
"none"      =.3))+
labs(title="Linear Relationship Between Popularity and Extraversion for 100 Classes",
subtitle="The 6 with the most extreme relationship have been highlighted red and blue")
##### ----------------------------------------------------------------------------------------------
# Start estimating bayesian multilevel models
# First, intercept only
interceptonlymodeltest <- brm(popular ~ 1 + (1 | class),
data   = popular2data,
warmup = 100,
iter   = 200,
chains = 2,
init  = "random",
cores  = 2)  #the cores function tells STAN to make use of 2 CPU cores simultaneously instead of just 1.
summary(interceptonlymodeltest)
# Add more iterations
interceptonlymodel <- brm(popular ~ 1 + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(interceptonlymodel)
hyp <- "sd_class__Intercept^2 / (sd_class__Intercept^2 + sigma^2) = 0"
hypothesis(interceptonlymodel, hyp, class = NULL)
# Second, add the other first-level predictors (individual level)
model1 <- brm(popular ~ 1 + sex + extrav + (1|class),
data = popular2data,
warmup = 1000, iter = 3000,
cores = 2, chains = 2,
seed = 123) #to run the model
summary(model1)
model1tranformed <- ggs(model1) # cannot use
View(model1tranformed)
mcmc_plot(model1, type = "trace")
ggplot(filter(model1tranformed, Parameter %in% c("b_Intercept", "b_extrav", "b_sex")),
aes(x   = Iteration,
y   = value,
col = as.factor(Chain)))+
geom_line() +
geom_vline(xintercept = 1000)+
facet_grid(Parameter ~ . ,
scale  = 'free_y',
switch = 'y')+
labs(title = "Caterpillar Plots",
col   = "Chains")
# Check credible intervals
# Not working until ggs works
ggplot(filter(model1tranformed,
Parameter == "b_Intercept",
Iteration > 1000),
aes(x = value))+
geom_density(fill  = "yellow",
alpha = .5)+
geom_vline(xintercept = 0,
color  = "red",
size = 1)+
scale_x_continuous(name   = "Value",
limits = c(-1, 3)) +
geom_vline(xintercept = summary(model1)$fixed[1,3],
color = "blue",
linetype = 2) +
geom_vline(xintercept = summary(model1)$fixed[1,4],
color = "blue",
linetype = 2) +
theme_light() +
labs(title = "Posterior Density of Intercept")
ggplot(filter(model1tranformed, Parameter == "b_extrav", Iteration > 1000), aes(x = value))+
geom_density(fill = "orange", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, .6))+
geom_vline(xintercept = summary(model1)$fixed[3,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[3,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Extraversion")
ggplot(filter(model1tranformed, Parameter == "b_sex", Iteration > 1000), aes(x = value))+
geom_density(fill = "red", alpha = .5)+
geom_vline(xintercept = 0, col = "red", size = 1)+
scale_x_continuous(name = "Value", limits = c(-.2, 1.5))+
geom_vline(xintercept = summary(model1)$fixed[2,3], col = "blue", linetype = 2)+
geom_vline(xintercept = summary(model1)$fixed[2,4], col = "blue", linetype = 2)+
theme_light()+
labs(title = "Posterior Density of Regression Coefficient for Sex")
summary(model1)$fixed
install.packages("tidybayes")
install.packages("devtools")
install.packages("fpp3")
install.packages("lavaan")
# Load required packages
library(tidyverse)
library(brms)
library(bayesplot)
# Generate simulated data
set.seed(23)
n <- 100
temperature <- runif(n, min = 0, max = 30) # Random temperatures between 0 and 30
intercept <- 10
slope <- 0.5
noise_sd <- 3
# round to whole numbers
bike_thefts <- (intercept + slope * temperature + rnorm(n, sd = noise_sd)) %>%
round(., 0)
data <- tibble(temperature = temperature, bike_thefts = bike_thefts)
View(data)
# Some initial descriptions
summary(data)
head(data)
library(ggplot2)
ggplot(data, aes(x = temperature, y = bike_thefts)) +
geom_point() +
labs(x = "Temperature", y = "Bike thefts") +
theme_minimal()
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
# --------------------------------------------------------------------------------------------------
# Start Bayes analysis
# 1) set the priors
priors <- prior(normal(10, 5), class = "Intercept") +
prior(normal(0, 1), class = "b")
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
library(rstantools)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
pp_check(fit, type = "dens_overlay", prefix = "ppd", ndraws = 100)
# 2) Specify and fit the model, note the sample_prior = "only"
# this makes sure we are going to look at the prior predictive samples.
fit <- brm(bike_thefts ~ temperature, data = data,
prior = priors, family = gaussian(),
sample_prior = "only", seed = 555)
sqrt(0.1)
load("~/GitHub/OrdinalSim/Results/Times/CatTimeIgnRow87Rep1.Rdata")
ctime.ign.cat
1102.64/60
load("~/GitHub/OrdinalSim/Results/Times/CatTimeRow87Rep1.Rdata")
ctime.cat
426.31/60
20*17400
install.packages("doSNOW")
library(doSNOW)
source("~/GitHub/OrdinalSim/do_sim.R", echo=TRUE)
source("~/GitHub/OrdinalSim/parallel_sim.R", echo=TRUE)
rm(results)
# Get object names
obj <- objects()
# Setup parallel cluster
cl <- makeCluster(2)
registerDoSNOW(cl)
iterations <- 100
pb <- txtProgressBar(max = iterations, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
# Export the necessary function and variables to the cluster
clusterEvalQ(cl, {
library(lavaan)
library(MASS)
library(combinat)
})
clusterEvalQ(cl, setwd("C:/Users/perezalo/Documents/GitHub/OrdinalSim/Results"))
clusterExport(cl, varlist = obj)
>clusterExport
?clusterExport
parallel::clusterExport(cl, varlist = obj)
# Parallel execution over RowDesign
# results <- parLapply(cl = cl, X = 1:2, fun = do_sim)
results <- foreach(RowDesign = 1:2, .options.snow = opts) %dopar% {
do_sim(RowDesign)
}
stopCluster(cl)
source("~/GitHub/OrdinalSim/parallel_sim.R", echo=TRUE)
source("~/GitHub/OrdinalSim/parallel_sim.R", echo=TRUE)
# __________________________________________________________________________________________________
# Start parallelization --------------------------------------------------------------------------
# __________________________________________________________________________________________________
library(foreach)
library(parallel)
library(doParallel)
library(doSNOW)
rm(results)
# Get object names
obj <- objects()
# Setup parallel cluster
cl <- makeCluster(2)
registerDoSNOW(cl)
iterations <- 100
pb <- txtProgressBar(max = iterations, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
# registerDoParallel(cl)
# Divide rows per cluster
# rows_divided <- split(1:2, 1:2)
# Export the necessary function and variables to the cluster
clusterEvalQ(cl, {
library(lavaan)
library(MASS)
library(combinat)
})
clusterEvalQ(cl, setwd("C:/Users/perezalo/Documents/GitHub/OrdinalSim/Results"))
parallel::clusterExport(cl, varlist = obj)
# Parallel execution over RowDesign
# results <- parLapply(cl = cl, X = 1:2, fun = do_sim)
results <- foreach(RowDesign = 1:2, .options.snow = opts) %dopar% {
do_sim(RowDesign)
}
stopCluster(cl)
load("C:/Users/perezalo/Documents/GitHub/OrdinalSim/Results/Ignored/ResultIgnRow2.Rdata")
View(ResultsRow.ign)
load("C:/Users/perezalo/Documents/GitHub/OrdinalSim/Results/Ignored/MM_ResultIgnRow2.Rdata")
View(ResultsRow_MM.ign)
